---
layout:     post
title:      "Computing approximate solutions for regressions on panel data efficiently: a new method"
date:       2020-11-10 10:00:00
summary:    "Coresets can efficiently summarize data for common statistics and machine learning algorithms; a new paper shows how to construct coresets for panel data."
author:     Chris Hays
author_link: https://johnchrishays.com/
image: /images/first-example.png
author_bio: Chris Hays is a Research Affiliate with the <a href="https://computationsociety.yale.edu/">Computation and Society Initiative</a> at Yale. He recently graduated from Yale with a degree in computer science. 
published: false
---

With the rise of big data, new techniques are needed to efficiently run algorithms for statistics and machine learning. One such strategy for dealing with large datasets is to use [coresets](https://en.wikipedia.org/wiki/Coreset). A coreset is a small weighted subset of a dataset that approximates the full data with respect to a certain algorithm; coresets can be constructed such that the output of the algorithm run on the coreset approximates that of the full data. It is often significantly faster to find a coreset and run the algorithm on it than to run the algorithm on the full data.

A [new paper](https://arxiv.org/abs/2011.00981) from Lingxiao Huang (Huawei TCS Lab, formerly a postdoc at Yale), [K. Sudhir](https://som.yale.edu/faculty/k-sudhir) (Yale School of Management) and [Nisheeth Vishnoi](http://www.cs.yale.edu/homes/vishnoi/Home.html) (Yale Computer Science) shows how to construct coresets for several types of regression on [panel data](https://en.wikipedia.org/wiki/Panel_data). Panel data comes from repeated observations of individuals, firms, etc. over time (as opposed to cross-sectional data, where there each individual, firm, etc. is only measured once). Coresets for cross-sectional data have been [studied](https://arxiv.org/abs/1202.3505) [extensively](https://arxiv.org/abs/1408.5099) [in](https://dl.acm.org/doi/10.5555/1109557.1109682) [the](https://arxiv.org/abs/1906.04705) [literature](https://arxiv.org/abs/1211.2713), but until the new paper, this work had not been extended to the panel data setting. Finding coresets for panel data is challenging because the number of observations per individual $$T$$ can be very large and we may want to account for correlations between observations within individuals.

In the next section, we'll discuss what makes coresets a useful tool and how to find them. Then we'll review the three types of regression for which constructing coresets from panel data is newly possible: ordinary least squares, generalized least squares, and a clustering extension of generalized least squares. Next, we'll discuss the main proof techniques behind the new research and, finally, discuss how well the algorithms for finding coresets for regression on panel data perform in practice.

## Why are coresets useful, and how do we find them?

There are two primary characteristics of coresets that make them useful.

1. *They come with provable guarantees about how well they will approximate the full data.* An $$\varepsilon$$-approximate coreset for a given algorithm guarantees that the value of the algorithm's [loss function](https://en.wikipedia.org/wiki/Loss_function) for the coreset and any parameters will be within a factor of $$(1 \pm \varepsilon)$$ of the value for the full data. This means that the output of the algorithm on the coreset must be close to that of the full data.

2. *They are small and fast to compute.* Algorithms to construct coresets (usually) depend on $$\varepsilon$$, but not the size of the data. This means that they can represent very large datasets with only a tiny subset of points. They also take linear time in the size of the data to compute. This means that, as the number of data points grows, the run time to find the coreset grows linearly with the size of the input. By contrast, the run time for many statistics and machine learning problems grows polynomially (for some polynomial function of the size of the dataset with degree $$>1$$) with the size of the input. This means that it is often significantly faster to find the coreset and run the algorithm on the coreset than to run the algorithm on the full data. 

There has been rapid development in the theory of coresets in the last decade. In 2011, [Feldman and Langberg](http://people.csail.mit.edu/dannyf/stoc11.pdf) established a framework for constructing coresets for a large class of algorithms. In general terms, let's walk through the intuition behind the Feldman-Langberg framework.

<table>
<tbody>
<tr>
<td markdown="span" style="border: 0px;">![Example data set]({{ site.baseurl }}/images/coresets/coreset1.png "Example data set"){:class="coreset"}</td>
<td markdown="span" style="border: 0px;">![Sampling distribution]({{ site.baseurl }}/images/coresets/coreset2.png "Sampling distribution"){:class="coreset"}</td>
<td markdown="span" style="border: 0px;">![Coreset]({{ site.baseurl }}/images/coresets/coreset3.png "Coreset"){:class="coreset"}</td>
</tr>
<tr>
<td markdown="span" style="border: 0px; font-size: 80%; text-align: center;"> (a) Example data set</td>
<td markdown="span" style="border: 0px; font-size: 80%; text-align: center;"> (b) Sampling distribution</td>
<td markdown="span" style="border: 0px; font-size: 80%; text-align: center;"> (c) Coreset</td>
</tr>
</tbody>
</table>
An example process for constructing a coreset. Start with the data, assign probabilities to points based on the sensitivity function, and then chose sufficiently many points by sampling from the points according to their probabilities. The small blue dots are the original data. For the large yellow/red dots, radius indicates probability and color indicates weight (red = high weight). These plots come from [Feldman et al, 2011](https://papers.nips.cc/paper/2011/file/2b6d65b9a9445c4271ab9076ead5605a-Paper.pdf).
{: style="color:gray; font-size: 80%;"}

We start with a dataset and a particular algorithm for which we want the coreset to approximate the dataset. For an example, we can consider the dataset in plot (a) of the figure above. We first assign a probability to each point and then sample a certain number of points from the dataset to construct the coreset. The probability assigned to a point is based on its *sensitivity*, or how much it would affect the loss function of the algorithm. As we can observe in (b) of the figure above, points that are outliers or are far away from other points are assigned higher probability, since they have higher sensitivity. Each point is also assigned a *weight*, which determines how much influence it will have on the algorithm (higher weight = more influence). The weights are generally assigned inversely proportional to sensitivity, such that outliers have the least weight and more representative points have more weight. Finally, once we sample sufficiently many points, we have our coreset!

Next, we discuss the three types of regression in the paper by Huang et al.

## A refresher on regression: ordinary least squares, generalized least squares, a clustering extension for generalized least squares

Perhaps the simplest way to deal with panel data is to assume there is no correlation within or between individuals. For $$N$$ individuals, $$T$$ time points and $$d$$ features, the data can be modeled as 

$$ y_{it} = x_{it}^\top \beta + \varepsilon_{it} $$
{: .center-eq }

where $$y_{it} \in \mathbb{R}^d$$ are the outcomes, $$x_{it} \in \mathbb{R}^d$$ are the predictors, and $$\varepsilon_{it}$$ are the error terms drawn from the normal distribution for individual $$i$$ at time point $$t$$. The vector $$\beta \in \mathbb{R}^d$$ consists of the regression coefficients to be estimated. Under this model, finding the optimal $$\beta$$ reduces to minimizing the loss function 

$$ \psi (\beta) := \sum_{i \in [N]} \sum_{t \in [T]} \| y_{it} - x_{it}^\top \beta \|_2^2 $$ 
{: .center-eq }
where $$|| \cdot ||_2$$ is the [$$\ell_2$$-norm](https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm). The vector $$\beta$$ that minimizes this function is called the **ordinary least squares estimator (OLSE)**. Throughout this post, we will also sometimes refer to the individual-time loss function, which is just one term $$ \| y_{it} - x_{it}^\top \beta \|_2^2 $$ in the loss function where $$i \in [N]$$ and $$t \in [T]$$. We will label the individual-time loss function with $$ \psi_{it} (\beta) $$.

But what we would like to account for correlation within individuals? 

One of the ways to model correlation within individuals is called **autocorrelation**, in which each error term $$ e_{it} $$ is partially determined by a weighted sum of up to $$ q $$ preceding error terms $$ e_{i(t-q)}, \dots, e_{i(t-1)} $$. Specifically, with autoregression of order $$ q $$, there is a vector $$\rho \in \mathbb{R}^q$$ such that $$\| \rho \|_2 < 1 $$ and

$$
e_{it} = \sum_{j=1}^{\min\{ t-1, t-q \}} \rho_{j} e_{i(t-j)} + N(0, 1)
$$
{: .center-eq }

Then the goal is to minimize the loss function

$$
\sum_{i=1}^N (1 - \| \rho \|_2^2)(y_i1 - xi1^\top \beta)^2 + \sum_{t=2}^T \left( (y_{it} - x_{it}^\top \beta) - \sum_{j=1}^{\min\{ t-1, q \}} \rho_{j} (y_{jt} - x_{jt}^\top \beta)  \right)
$$
{: .center-eq }

over the parameters $$\beta, \rho$$. The vectors that minimize this object function are called **generalized least squares estimators (GLSE)**.

As an extension of GLSE, one might want to break individuals into clusters and run generalized least squares regression on the clusters. In this case, for $$k$$ clusters, there are $$k$$ regression coefficient vectors $$\beta^{(1)}, \dots , \beta^{(k)}$$ and $$k$$ autocorrleation parameter vectors $$\rho^{(1)}, … , \rho^{(k)}$$. The loss function for a given individual is

$$
(1 - \| \rho^{(\ell)} \|_2^2)(y_i1 - xi1^\top \beta^{(\ell)})^2 + \sum_{t=2}^T \left( (y_{it} - x_{it}^\top \beta^{(\ell)}) - \sum_{j=1}^{\min\{ t-1, q \}} \rho_{j}^{(\ell)} (y_{jt} - x_{jt}^\top \beta^{(\ell)})  \right)
$$
{: .center-eq }

for whichever $$\ell$$ yields the minimum value of the expression. The clustering extension of the generalized least squares estimator is abreviated GLSE$$_K$$.

The new paper allows the construction of coresets for the regression methods described above on panel data.

## The technical ideas behind finding a coreset for regression on panel data

As we saw above, the intuition behind the Feldman-Langberg framework is that the loss function is more “sensitive” to certain data points than others, so we should include points with high sensitivity in the coreset. This intuition can be made quantitative by defining a *sensitivity function*. It is also important to know how many points to sample from the data. Namely, we need to ensure that the size of the coreset does not depend on $$N$$ or $$T$$. Huang et al use [*pseudo-dimension*](https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_dimension#Generalizations) to show that the number of samples needed does not depend on $$N$$ or $$T$$.

![Resampling]({{ site.baseurl }}/images/coresets/resampling.png "resampling"){: .center-image }

The importance function is shown on the top part of the graph. Below, points are sampled from the graph with probability proportional to their importance. This diagram comes from [Happe et al, 2009](http://www.reconos.de/publications/happe09_arc.pdf)
{: style="color:gray; font-size: 80%;"}

Let us now define sensitivity and pseudo-dimension in more detail and discuss how they are important for establishing that the algorithm works, and how Huang et al find them.

The **sensitivity function** for individual $$i \in [N]$$ at time $$t \in [T]$$, in intuition, captures the degree to which a particular point has an influence on the total loss function. Technically, it is an upper bound on the [supremum](wiki link) of the individual-time loss function as a proportion of the total loss function over all parameters $$\beta, \rho$$:

$$ s (i,t) \geq \sup_{\beta, \rho} \frac{\psi_{it} (\beta, \rho)}{\psi (\beta, \rho)} $$
{: .center-eq }

Finding a suitable sensitivity function for GLS regression is more difficult than for OLS regression (for which there was already a solution in the literature) because the GLS loss function is non-convex. Thus, finding a suitable sensitivity function involves developing a relationship between the sensitivities of GLS estimators and OLS estimators, and then using the fact that a suitable sensitivity function for OLS regression is known to upper bound the sensitivity function for GLS regression. Their proof goes analogously for GLSE_K, using the relationship to OLSE_K.

The **pseudo-dimension** is the other important feature of Huang et al’s construction of the regression coresets. Pseudo-dimension is a feature of the **query space**, which consists of the data, the queries (i.e. range of possible parameters) and the loss function associated with the analysis. It tells us how many samples we need to take over the data to find a coreset with high probability. Thus, it is necessary to show that the pseudo-dimesion of the regression query spaces of interest does not depend on the number of individuals $$N$$ or the number of sampled time points $$T$$. Huang et al show that the pseudo-dimension of GLSE and GLSE_K depends on the number of regression parameters $$d$$ and the autoregression order $$q$$, using results from previous research on the pseudo-dimension of feed-forward neural networks.

Next, we discuss how these algorithms perform in practice. 


## How well does it perform in practice?

One major remaining question about the corset regression algorithms is how well it performs in practice. Huang et al test their algorithm for constructing coresets on several datasets:
* A synthetic dataset sampled from a multivariate normal distribution.
* A synthetic dataset sampled from a Cauchy distribution, which has heavy tails and undefined expectation and variance. The Cauchy distribution generates “outliers” with greater frequency than the normal distribution, and is therefore an interesting test of how the coreset algorithm performs on less well-behaved data.
* A real-world data set of the profits earned from card holders of a credit card holder, using individuals’ demographics, past behavior, credit balance and fees paid to predict the profit they might yield.

As a baseline, they compared how the coresets constructed by their algorithm compared to a random sample of the data of the same size as the coreset.

First, let’s discuss the practical trade-offs between guaranteed accuracy and coreset size. For the synthetic datasets using the normal and Cauchy distributions, the 0.1-coreset (a coreset producing results in a factor of 0.9 to 1.1 times the results run on the full data) has size less than one half than the full data, while for the 0.1-coreset of the real-world data, the coreset size is about fifth of the size of the original data. The maximum empirical error for all three tested datasets was always less than the worst-case for random sampling. They also tested $$\varepsilon = 0.2, 0.3, 0.4, 0.5$$, which yielded similar patterns. Using $$\varepsilon = 0.5$$ yielded a coreset of size less than 1% of the size of the data.

The smaller the size of the data, the more efficiently the regression can be computed on the data. This means that finding the coreset and then running the regression on the coreset can be significantly faster than running the regression on the full data. On the datasets they tested, finding the coreset and running the regression on it was faster than running the regression on the full data by a factor of 1.2-108.

Tshe coresets performed significantly better on the data set generated from the Cauchy distribution, compared to randomly sampling from the data. This indicates that the coreset was able to capture the outliers in the data better than randomly sampling coule.

You can view the result of their empirical results in Table 1 of the paper.

## Conclusion

Researchers have made rapid progress in expanding the theory of coresets for various statistics and machine learning models. This paper extends the robust theory of coresets for regression to the panel data setting. Panel data is widely used in applications, so this paper presented an exciting contribution to the existing research on the subject.

An important limitation of the paper (and the theory of coresets in general) is that the coresets produced for a particular algorithm does not come with any guarantees about other algorithms. Thus, training a machine learning model using something other than regression on a coreset for regression may yield misleading results. Thus, coresets are not necessarily good general-purpose summaries of data. 

Future developments in this area of research may yield new coresets for different algorithms in statistics and machine learning. 
